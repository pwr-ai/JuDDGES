vars:
  - seeds: [42, 7312, 997]

stages:
  raw_dataset_readme:
    cmd: >-
      jupyter nbconvert
      --no-input
      --to markdown
      --execute 'nbs/Dataset Cards/01_Dataset_Description_Raw.ipynb'
      --output-dir data/datasets/pl/pl-court-raw
      --output README
    deps:
      - nbs/Dataset Cards/01_Dataset_Description_Raw.ipynb
      - data/datasets/pl/pl-court-raw/data/
    outs:
      - data/datasets/pl/pl-court-raw/README.md
      - data/datasets/pl/pl-court-raw/README_files/

  embed:
    matrix:
      model:
        - mmlw-roberta-large
    cmd: >-
      PYTHONPATH=. python scripts/embed/embed_text.py embedding_model=${item.model}
    deps:
      - scripts/embed/embed_text.py
      - configs/embedding.yaml
      - configs/embedding_model/${item.model}.yaml
      - data/datasets/pl/raw
    outs:
      - data/embeddings/pl-court-raw/${item.model}/all_embeddings

  aggregate_embeddings:
    matrix:
      model:
        - mmlw-roberta-large
    cmd: >-
      PYTHONPATH=. python scripts/embed/aggregate_embeddings.py
      --embeddings-dir data/embeddings/pl-court-raw/${item.model}/all_embeddings
    deps:
      - scripts/embed/aggregate_embeddings.py
      - data/embeddings/pl-court-raw/${item.model}/all_embeddings
    outs:
      - data/embeddings/pl-court-raw/${item.model}/agg_embeddings.pt

  build_graph_dataset:
    cmd: >-
      PYTHONPATH=. python scripts/dataset/build_graph_dataset.py
      --dataset-dir data/datasets/pl/raw
      --embeddings-root-dir data/embeddings/pl-court-raw/mmlw-roberta-large/
      --target-dir data/datasets/pl/graph
    deps:
      - scripts/dataset/build_graph_dataset.py
      - juddges/data/pl_court_graph.py
      - data/datasets/pl/raw
      - data/embeddings/pl-court-raw/mmlw-roberta-large/agg_embeddings.pt
      - data/embeddings/pl-court-raw/mmlw-roberta-large/all_embeddings/config.yaml
    outs:
      - data/datasets/pl/graph/data
      - data/datasets/pl/graph/metadata.yaml

  build_swiss_franc_loans_instruct_dataset:
    cmd: >-
      PYTHONPATH=. python scripts/dataset/build_swiss_franc_loans_instruct_dataset.py
      --train-ds-path data/analysis/sprawy_frankowe/extractions_df_2024-12-04.pkl
      --test-ds-path data/analysis/sprawy_frankowe/extractions_df_2024-12-11_test.pkl
      --output-dir data/datasets/pl/swiss_franc_loans
      --tokenizer-name meta-llama/Llama-3.1-8B-Instruct
      --threshold-tokens 64_000
    deps:
      - scripts/dataset/build_swiss_franc_loans_instruct_dataset.py
      - data/analysis/sprawy_frankowe/extractions_df_2024-12-04.pkl
      - data/analysis/sprawy_frankowe/extractions_df_2024-12-11_test.pkl
    outs:
      - data/datasets/pl/swiss_franc_loans/train.jsonl
      - data/datasets/pl/swiss_franc_loans/test.jsonl
      - data/datasets/pl/swiss_franc_loans/dataset_info.json

  ### ---------------------------- Fine-tuning ---------------------------- ###
  sft:
    matrix: &sft_models
      llm_dataset:
        - dataset: pl_court_swiss_franc_loans
          llm: llama_3.1_8b_instruct
          prompt: info_extraction_json
          ie_schema: swiss_franc_loans
        - dataset: pl_court_swiss_franc_loans
          llm: llama_3.2_3b_instruct
          prompt: info_extraction_json
          ie_schema: swiss_franc_loans
        - dataset: pl_court_swiss_franc_loans
          llm: mistral_nemo_instruct_2407
          prompt: info_extraction_json
          ie_schema: swiss_franc_loans
        - dataset: pl_court_swiss_franc_loans
          llm: pllum_12b_instruct
          prompt: info_extraction_json
          ie_schema: swiss_franc_loans
        - dataset: pl_court_swiss_franc_loans
          llm: phi_4
          prompt: info_extraction_json
          ie_schema: swiss_franc_loans
        - dataset: en-appealcourt-coded-instruct
          llm: llama_3.1_8b_instruct
          prompt: info_extraction_json
          ie_schema: swiss_franc_loans
    cmd: >-
      PYTHONPATH=. python scripts/sft/fine_tune_deepspeed.py
      dataset=${item.llm_dataset.dataset}
      llm=${item.llm_dataset.llm}
      prompt=${item.llm_dataset.prompt}
      ie_schema=${item.llm_dataset.ie_schema}
    deps:
      - scripts/sft/fine_tune_deepspeed.py
      - configs/peft_fine_tuning.yaml
      - configs/llm/${item.llm_dataset.llm}.yaml
    outs:
      - data/experiments/peft-fine-tune/${item.llm_dataset.llm}/${item.llm_dataset.dataset}/${item.llm_dataset.prompt}/${item.llm_dataset.ie_schema}/

  ### ---------------------------- Prediction ---------------------------- ###
  predict_swiss_franc_loans:
    matrix:
      <<: *sft_models
      seed: ${seeds}
    cmd: >-
      PYTHONPATH=. python scripts/sft/predict.py
      dataset=${item.llm_dataset.dataset}
      llm=${item.llm_dataset.llm}
      prompt=${item.llm_dataset.prompt}
      ie_schema=${item.llm_dataset.ie_schema}
      random_seed=${item.seed}
    deps:
      - scripts/sft/predict.py
      - configs/predict.yaml
      - configs/llm/${item.llm_dataset.llm}.yaml
      - configs/prompt/${item.llm_dataset.prompt}.yaml
      - configs/ie_schema/${item.llm_dataset.ie_schema}.yaml
    outs:
      - data/experiments/predict/raw/${item.llm_dataset.dataset}/${item.llm_dataset.llm}/${item.llm_dataset.prompt}/${item.llm_dataset.ie_schema}/${item.seed}/outputs.json
      - data/experiments/predict/raw/${item.llm_dataset.dataset}/${item.llm_dataset.llm}/${item.llm_dataset.prompt}/${item.llm_dataset.ie_schema}/${item.seed}/dataset.json

  predict_on_fine_tuned:
    matrix:
      llm_dataset:
        - dataset: pl-court-frankowe-instruct
          llm: llama_3.2_3b_instruct
      seed: ${seeds}
    cmd: >-
      PYTHONPATH=. python scripts/sft/predict.py
      dataset=${item.llm_dataset.dataset}
      llm=${item.llm_dataset.llm}
      llm.adapter_path=data/experiments/fine-tune/${item.llm_dataset.dataset}/${item.llm_dataset.llm}
      random_seed=${item.seed}
      output_file=data/experiments/predict/${item.llm_dataset.dataset}/${item.llm_dataset.llm}/outputs_${item.seed}.json
    deps:
      - scripts/sft/predict.py
      - configs/predict.yaml
      - configs/llm/${item.llm_dataset.llm}.yaml
    outs:
      - data/experiments/predict/${item.llm_dataset.dataset}/${item.llm_dataset.llm}/outputs_${item.seed}.json

  predict_with_api:
    matrix:
      dataset:
        - pl-court-instruct
        - en-court-instruct
      llm:
        - gpt-4o
        - gpt-4o-mini
      seed:
        - 997
    cmd: >-
      PYTHONPATH=. python scripts/sft/predict_with_api.py
      dataset=${item.dataset}
      llm=${item.llm}
      seed=${item.seed}
      output_file=data/experiments/predict/${item.dataset}/open_ai_${item.llm}/outputs_${item.seed}.json
    deps:
      - scripts/sft/predict_with_api.py
      - configs/predict_with_api.yaml
    outs:
      - data/experiments/predict/${item.dataset}/open_ai_${item.llm}/outputs_${item.seed}.json

  ### ---------------------------- Evaluation ---------------------------- ###
  evaluate: # use with NUM_PROC
    matrix:
      <<: *sft_models
      seed: ${seeds}
    cmd: >-
      PYTHONPATH=. python scripts/sft/evaluate.py
      --output-file data/experiments/predict/raw/${item.llm_dataset.dataset}/${item.llm_dataset.llm}/outputs_${item.seed}.json
      --num-proc=-1
    deps:
      - scripts/sft/evaluate.py
      - data/experiments/predict/raw/${item.llm_dataset.dataset}/${item.llm_dataset.llm}/outputs_${item.seed}.json
    outs:
      - data/experiments/predict/raw/${item.llm_dataset.dataset}/${item.llm_dataset.llm}/metrics_${item.seed}.json

  evaluate_api_models:
    matrix:
      dataset:
        - en-court-instruct
        - pl-court-instruct
      llm:
        - open_ai_gpt-4o
        - open_ai_gpt-4o-mini
      seed: [997]
    cmd: >-
      PYTHONPATH=. python scripts/sft/evaluate.py
      --output-file data/experiments/predict/${item.dataset}/${item.llm}/outputs_${item.seed}.json
      --num-proc=-1
    deps:
      - scripts/sft/evaluate.py
      - data/experiments/predict/${item.dataset}/${item.llm}/outputs_${item.seed}.json
    outs:
      - data/experiments/predict/${item.dataset}/${item.llm}/metrics_${item.seed}.json

  evaluate_llm_as_judge_pl:
    matrix:
      judge_llm:
        - gpt_4o_mini
      evaluated_llm:
        - Unsloth-Llama-3-8B-Instruct
        - Unsloth-Llama-3-8B-Instruct-fine-tuned
        - Unsloth-Mistral-Nemo-Instruct-2407
        - Unsloth-Mistral-Nemo-Instruct-2407-fine-tuned
        - Bielik-7B-Instruct-v0.1
        - Bielik-7B-Instruct-v0.1-fine-tuned
        - Bielik-11B-v2.2-Instruct
        - Bielik-11B-v2.2-Instruct-fine-tuned
      seed: ${seeds}
    cmd: >-
      PYTHONPATH=. python scripts/sft/evaluate_llm_as_judge.py
      api_llm=${item.judge_llm}
      answers_file=data/experiments/predict/pl-court-instruct/${item.evaluated_llm}/outputs_${item.seed}.json
      out_metric_file=data/experiments/predict/pl-court-instruct/${item.evaluated_llm}/judge_metrics_${item.seed}.json
      prompt=pl
    deps:
      - scripts/sft/evaluate_llm_as_judge.py
      - data/experiments/predict/pl-court-instruct/${item.evaluated_llm}/outputs_${item.seed}.json
    outs:
      - data/experiments/predict/pl-court-instruct/${item.evaluated_llm}/judge_metrics_${item.seed}.json

  evaluate_llm_as_judge_en:
    matrix:
      judge_llm:
        - gpt_4o_mini
      evaluated_llm:
        - Unsloth-Llama-3-8B-Instruct
        - Unsloth-Llama-3-8B-Instruct-fine-tuned-en
        - Unsloth-Mistral-Nemo-Instruct-2407
        - Unsloth-Mistral-Nemo-Instruct-2407-fine-tuned-en
      seed: ${seeds}
    cmd: >-
      PYTHONPATH=. python scripts/sft/evaluate_llm_as_judge.py
      api_llm=${item.judge_llm}
      answers_file=data/experiments/predict/en-court-instruct/${item.evaluated_llm}/outputs_${item.seed}.json
      out_metric_file=data/experiments/predict/en-court-instruct/${item.evaluated_llm}/judge_metrics_${item.seed}.json
      prompt=en
    deps:
      - scripts/sft/evaluate_llm_as_judge.py
      - data/experiments/predict/en-court-instruct/${item.evaluated_llm}/outputs_${item.seed}.json
    outs:
      - data/experiments/predict/en-court-instruct/${item.evaluated_llm}/judge_metrics_${item.seed}.json

  evaluate_llm_as_judge_api_models:
    matrix:
      language:
        - pl
        - en
      judge_llm:
        - gpt_4o_mini
      evaluated_llm:
        - open_ai_gpt-4o
        - open_ai_gpt-4o-mini
      seed: [997]
    cmd: >-
      PYTHONPATH=. python scripts/sft/evaluate_llm_as_judge.py
      api_llm=${item.judge_llm}
      answers_file=data/experiments/predict/${item.language}-court-instruct/${item.evaluated_llm}/outputs_${item.seed}.json
      out_metric_file=data/experiments/predict/${item.language}-court-instruct/${item.evaluated_llm}/judge_metrics_${item.seed}.json
      prompt=${item.language}
    deps:
      - scripts/sft/evaluate_llm_as_judge.py
      - data/experiments/predict/${item.language}-court-instruct/${item.evaluated_llm}/outputs_${item.seed}.json
    outs:
      - data/experiments/predict/${item.language}-court-instruct/${item.evaluated_llm}/judge_metrics_${item.seed}.json

  summarize_ngram_metrics:
    matrix:
      dir:
        - data/experiments/predict/raw/pl-court-frankowe-instruct
    cmd: >-
      PYTHONPATH=. python scripts/sft/summarize_metrics.py
      --root-dir ${item.dir}
    deps:
      - scripts/sft/summarize_metrics.py
    metrics:
      - ${item.dir}/metrics_ngram_summary.md:
          cache: false

  summarize_judge_metrics:
    matrix:
      dir:
        - data/experiments/predict/raw/pl-court-frankowe-instruct
    cmd: >-
      PYTHONPATH=. python scripts/sft/summarize_metrics.py
      --root-dir ${item.dir}
    deps:
      - scripts/sft/summarize_metrics.py
    metrics:
      - ${item.dir}/metrics_judge_summary.md:
          cache: false
