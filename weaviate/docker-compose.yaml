name: legal_ai

services:
  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - '8080'
      - --scheme
      - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.30.2
    depends_on:
      - t2v-transformers-base
      - t2v-transformers-dev
      - t2v-transformers-fast
    ports:
      - 8084:8080
      - 50051:50051
    volumes:
      - legal_ai_weaviate_prod:/var/lib/weaviate
    env_file: .env
    restart: on-failure
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
      resources:
        limits:
          cpus: '25'
          memory: '60G'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Base model transformer service (mmlw-roberta-large)
  t2v-transformers-base:
    build:
      context: .
      dockerfile: hf_transformers.dockerfile
      args:
        MODEL_NAME: 'sdadas/mmlw-roberta-large'
        ENABLE_CUDA: ${ENABLE_CUDA}
    ports:
      - "8080:8080"
    environment:
      ENABLE_CUDA: ${ENABLE_CUDA}
      MODEL_NAME: 'sdadas/mmlw-roberta-large'
    restart: on-failure
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
      resources:
        limits:
          cpus: '8'
          memory: '16G'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Dev model transformer service (all-mpnet-base-v2)
  t2v-transformers-dev:
    build:
      context: .
      dockerfile: hf_transformers.dockerfile
      args:
        MODEL_NAME: 'sentence-transformers/all-mpnet-base-v2'
        ENABLE_CUDA: ${ENABLE_CUDA}
    ports:
      - "8081:8080"
    environment:
      ENABLE_CUDA: ${ENABLE_CUDA}
      MODEL_NAME: 'sentence-transformers/all-mpnet-base-v2'
    restart: on-failure
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
      resources:
        limits:
          cpus: '8'
          memory: '16G'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Fast model transformer service (all-MiniLM-L6-v2)
  t2v-transformers-fast:
    build:
      context: .
      dockerfile: hf_transformers.dockerfile
      args:
        MODEL_NAME: 'sentence-transformers/all-MiniLM-L6-v2'
        ENABLE_CUDA: ${ENABLE_CUDA}
    ports:
      - "8082:8080"
    environment:
      ENABLE_CUDA: ${ENABLE_CUDA}
      MODEL_NAME: 'sentence-transformers/all-MiniLM-L6-v2'
    restart: on-failure
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
      resources:
        limits:
          cpus: '4'
          memory: '8G'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  legal_ai_weaviate_prod:
    external: true
