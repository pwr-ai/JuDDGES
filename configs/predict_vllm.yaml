defaults:
  - llm: ???
  - dataset: ???
  - prompt: ???
  - ie_schema: ???
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

llm:
  batch_size: 1 # always use the same batch_size for reproducibility

prompt:
  language: ${dataset.language}
  ie_schema: ${ie_schema}

device_map: 'auto'
output_dir: data/experiments/predict/raw_vllm/${hydra:runtime.choices.dataset}/${hydra:runtime.choices.llm}/${hydra:runtime.choices.prompt}/${hydra:runtime.choices.ie_schema}/seed_${random_seed}
truncate_context: True
generate_kwargs:
  max_tokens: ${dataset.max_output_tokens}
  temperature: 0.4
  top_p: 0.8
  top_k: 20
  min_p: 0.0

split: ???
random_seed: ???
max_model_len: null # by default inferred from model config, but sometimes rope scaling is advised anyway
parallel: null # either 'tensor' or 'pipeline' (former for DDP-like, latter for pipeline parallelism)

hydra:
  output_subdir: null
  run:
    dir: .
