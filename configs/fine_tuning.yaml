defaults:
  - model: ???
  - dataset: pl-court-instruct
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

training_args:
  run_name: ${run_name}
  output_dir: ${output_dir}
  num_train_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: True
  gradient_checkpointing_kwargs: { "use_reentrant": False }
  optim: "adamw_torch_fused"
  logging_steps: 1
  save_strategy: "steps"
  save_steps: 500
  bf16: True
  learning_rate: 2e-5 # learning rate, based on QLoRA paper
  max_grad_norm: 0.3 # max gradient norm based on QLoRA paper
  warmup_ratio: 0.03 # warmup ratio based on QLoRA paper
  push_to_hub: False
  report_to: "wandb"
  seed: 46

peft_args:
  r: 8
  lora_alpha: 16
  lora_dropout: 0 # Supports any, but = 0 is optimized
  bias: "none" # Supports any, but = "none" is optimized
  target_modules: "all-linear"
    #[
     # "q_proj",
     # "k_proj",
     # "v_proj",
     # "o_proj",
     # "gate_proj",
     # "up_proj",
     # "down_proj",
    #]

truncate_context: True
wandb_entity: ebi_literature
wandb_project: juddges-fine-tune

output_dir: data/experiments/fine-tune/${hydra:runtime.choices.model}/${hydra:runtime.choices.dataset}
run_name: ${hydra:runtime.choices.model}_${hydra:runtime.choices.dataset}_fine_tune

hydra:
  output_subdir: null
  run:
    dir: .
