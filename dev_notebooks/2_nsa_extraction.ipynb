{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from juddges.settings import NSA_DATA_PATH\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "lf = pl.scan_parquet(NSA_DATA_PATH / \"pages\" / \"pages_chunk_0.parquet\")"
   ],
   "id": "7aea733cd6d05a0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = lf.collect()",
   "id": "fd8407a0c08061c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# df[2][\"page\"]",
   "id": "2e10800d536614a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "page = df[2][\"page\"].item()",
   "id": "da7fdf1707d6844e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d0627a9bf8383808"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5d30eac358952608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from itertools import groupby\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "\n",
    "# Function to split text by <br/> tags\n",
    "def extract_data_with_br_tags(value):\n",
    "    return [item.strip() for item in value.split('<br/>') if item.strip()]\n",
    "\n",
    "# Function to preserve paragraphs with \\n\\n\n",
    "def extract_text_preserve_paragraphs(html_element):\n",
    "    paragraphs = html_element.find_all(['p', 'br'])\n",
    "    text_parts = []\n",
    "    for paragraph in paragraphs:\n",
    "        text = paragraph.get_text(strip=True)\n",
    "        if text:\n",
    "            text_parts.append(text)\n",
    "    return '\\n\\n'.join(text_parts)\n",
    "\n",
    "def extract_przepisy(value):\n",
    "    chunks = []\n",
    "    x = 0\n",
    "    chunks.append([])   # create an empty chunk to which we'd append in the loop\n",
    "    for i in value.contents:\n",
    "        if ('<br/>' not in str(i)):\n",
    "            if i.get_text(strip=True) != '':\n",
    "                chunks[x].append(i)\n",
    "        else:\n",
    "            x += 1\n",
    "            chunks.append([])\n",
    "    \n",
    "    reference = chunks[0::2]\n",
    "    ustawa = chunks[1::2]\n",
    "    \n",
    "    dziennik_ustaw = [r[0].get_text(strip=True) for r in reference]\n",
    "    art = [r[1].get_text(strip=True) if len(r) > 1 else None for r in reference ]\n",
    "    links = [r[0][\"href\"] if isinstance(r[0], Tag) else None for r in reference]\n",
    "    ustawa = [u[0].get_text(strip=True) for u in ustawa]\n",
    "    \n",
    "    assert len(dziennik_ustaw) == len(links) == len(art) == len(ustawa)\n",
    "\n",
    "    return {\n",
    "        \"Dziennik ustaw\": dziennik_ustaw,\n",
    "        \"Dziennik ustaw link\": links,\n",
    "        \"Artykuły\": art,\n",
    "        \"Ustawa\": ustawa\n",
    "    }\n",
    "\n",
    "def extract_data(page):\n",
    "    # Initialize BeautifulSoup with the HTML content\n",
    "    soup = BeautifulSoup(page, 'html.parser')  # 'page' contains the HTML\n",
    "    \n",
    "    # Dictionary to store the extracted data\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Find all rows containing labels and values\n",
    "    rows = soup.find_all('tr', class_='niezaznaczona')\n",
    "    \n",
    "    # Iterate through the rows and extract label-value pairs\n",
    "    for row in rows:\n",
    "        label = row.find('td', class_='lista-label')\n",
    "        value = row.find('td', class_='info-list-value')\n",
    "    \n",
    "        if label and value:\n",
    "            label_text = label.get_text(strip=True)\n",
    "            value_text = value.decode_contents().strip()  # Extract HTML content, including <br/>\n",
    "    \n",
    "            if 'Powołane przepisy' in label_text:\n",
    "                extracted_data = extracted_data | extract_przepisy(value)\n",
    "            elif 'Sygn. powiązane' in label_text:\n",
    "                 extracted_data[label_text] = [a['href'] for a in value.find_all('a')]\n",
    "                 extracted_data[label_text] = [a.get_text(strip=True) for a in value.find_all('a')]\n",
    "                 extracted_data[label_text+\" link\"] = [a['href'] for a in value.find_all('a')]\n",
    "            elif '<br/>' in value_text or label_text in (\"Hasła tematyczne\", \"Symbol z opisem\", \"Sędziowie\", \"Treść wyniku\"):\n",
    "                extracted_data[label_text] = extract_data_with_br_tags(value_text)\n",
    "                if label_text == \"Sędziowie\":\n",
    "                    function = [re.findall(r'/([^/]*)/', j) for j in extracted_data[label_text]]\n",
    "                    # function = [f[0] if f else None for f in function]\n",
    "                    extracted_data[label_text] = [re.sub(r'/[^/]*/', '', s).strip() for s in extracted_data[label_text]]\n",
    "\n",
    "                    function_map = {f[0]:j for f, j in zip(function, extracted_data[label_text]) if f}\n",
    "                    if \"przewodniczący\" in function_map and \"sprawozdawca\" in function_map:\n",
    "                        extracted_data[\"przewodniczący\"] = function_map[\"przewodniczący\"]\n",
    "                        extracted_data[\"sprawozdawca\"] = function_map[\"sprawozdawca\"]\n",
    "                    elif \"przewodniczący sprawozdawca\" in function_map:\n",
    "                        extracted_data[\"przewodniczący\"] = function_map[\"przewodniczący sprawozdawca\"]\n",
    "                        extracted_data[\"sprawozdawca\"] = function_map[\"przewodniczący sprawozdawca\"]\n",
    "            elif 'Data orzeczenia' in label_text:\n",
    "                date_value = value.find_all('td')[0].get_text(strip=True)\n",
    "                judgement_type = value.find_all('td')[1].get_text(strip=True)\n",
    "                if len(judgement_type) == 0:\n",
    "                    judgement_type = None\n",
    "                extracted_data['Data orzeczenia'] = date_value\n",
    "                extracted_data['Rodzaj orzeczenia'] = judgement_type\n",
    "            else:\n",
    "                extracted_data[label_text] = value_text\n",
    "    \n",
    "    # Extract sections that follow the pattern in the document (avoiding explicit label names)\n",
    "    section_headers = soup.find_all('div', class_='lista-label')\n",
    "    for header in section_headers:\n",
    "        next_section = header.find_next('span', class_='info-list-value-uzasadnienie')\n",
    "        if next_section:\n",
    "            header_text = header.get_text(strip=True)\n",
    "            extracted_data[header_text] = extract_text_preserve_paragraphs(next_section)\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "l = []\n",
    "\n",
    "for item in tqdm(df[:500].iter_rows(named=True)):\n",
    "    data = extract_data(item[\"page\"])\n",
    "    data[\"doc_id\"] = item[\"doc_id\"]\n",
    "    data[\"page\"] = item[\"page\"]\n",
    "    l.append(data)"
   ],
   "id": "ec0c85fa029d48ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "l[0]",
   "id": "b81a6298867d878d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.DataFrame(l)",
   "id": "ddbb4144a15d8d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "1b3cbf67960cf4e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, entry in df.iterrows():\n",
    "    if not isinstance(entry[\"Dziennik ustaw\"], list):\n",
    "        continue\n",
    "    if not len(entry['Dziennik ustaw']) == len(entry[\"Dziennik ustaw link\"]) == len(entry[\"Artykuły\"]) == len(entry[\"Ustawa\"]):\n",
    "        print(entry)\n",
    "        print(entry[\"Dziennik ustaw\"])\n",
    "        print(entry[\"Dziennik ustaw link\"])\n",
    "        print(entry[\"Artykuły\"])\n",
    "        print(entry[\"Ustawa\"])\n",
    "        break"
   ],
   "id": "e2e2da85caed875e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "94518255c1fa0c68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "730b559c2a213c96",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuDDGES",
   "language": "python",
   "name": "juddges"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
